[2025-10-05 04:53:40,231][root][INFO] - Configuration: experiment:
  data_path: ../data
  checkpoint_dir: ../checkpoints
  models:
  - 'inf'
  row_fraction: 1.0
  num_targets: 424
  filter_features: false
  optuna:
    direction: minimize
    n_trials: 2
    n_jobs: 2
model:
  model_type: default
  model_class: default
  trainer_class: default
  params:
    input_size: 1672
    output_size: 424
    input_len: 64
    output_len: 5
    batch_size: 16
    hidden_size: 128
    num_layers: 2
    dropout: 0.1
    epochs: 10
    patience: 5
    channels: 64
    blocks: 3
    kernel_size: 5
    num_conv_layers: 2
    tcn_kernel: 3
    decom_kernel_size: 25
    top_k_modes: 5
    nhead: 8
    dim_feedforward: 512
    weight_decay: 0.001
    scheduler_patience: 5
    scheduler_factor: 0.5
    clip_grad_norm: 1.0
    partially_finite_target: false
  lr: 0.001
  optimizer: AdamW
  lr_policy: constant

[2025-10-05 04:53:40,232][root][INFO] - Set random seed to 42 for reproducibility.
[2025-10-05 04:53:40,232][root][INFO] - CUDA version: 12.1
[2025-10-05 04:53:40,271][root][INFO] - Device count: 1
[2025-10-05 04:53:40,274][root][INFO] - Device name: NVIDIA GeForce RTX 4060 Laptop GPU
[2025-10-05 04:53:40,367][root][INFO] - Loading train.csv from ../data/train.csv
[2025-10-05 04:53:40,448][root][INFO] - Loading train_labels.csv from ../data/train_labels.csv
[2025-10-05 04:53:40,560][root][INFO] - Loading target_pairs.csv from ../data/target_pairs.csv
[2025-10-05 04:53:40,717][root][INFO] - X_full NaNs after imputation: 0
[2025-10-05 04:53:40,717][root][INFO] - Y_full NaNs before imputation: 0
[2025-10-05 04:53:40,718][root][INFO] - Y_full NaNs after imputation: 0
[2025-10-05 04:53:40,718][root][INFO] - X_full feature count: 1671
[2025-10-05 04:53:40,728][root][INFO] - Data loaded: X_full_np shape=(1917, 1671), Y_full_np shape=(1917, 424)
[2025-10-05 04:53:40,729][root][INFO] - Raw data shapes: X=(1917, 1671), Y=(1917, 424)
[2025-10-05 04:53:40,729][root][INFO] - Sample batch shapes: X=(64, 1671), Y=(64, 424)
[2025-10-05 04:53:41,211][root][INFO] - Trial 16 started with params: {'input_size': 1671, 'output_size': 424, 'output_len': 5, 'input_len': 96, 'label_len': 64, 'hidden_size': 384, 'num_layers': 3, 'd_layers': 1, 'nhead': 4, 'dim_feedforward': 512, 'dropout': 0.15000000000000002, 'batch_size': 32, 'attn': 'prob', 'distil': True, 'mix': True}, lr=4.335309496700804e-05, optimizer=AdamW, lr_policy=clr
[2025-10-05 04:53:41,497][root][INFO] - Performed memory cleaning.
[2025-10-05 04:53:41,598][root][INFO] - Trial 17 started with params: {'input_size': 1671, 'output_size': 424, 'output_len': 5, 'input_len': 96, 'label_len': 64, 'hidden_size': 384, 'num_layers': 3, 'd_layers': 1, 'nhead': 4, 'dim_feedforward': 512, 'dropout': 0.15000000000000002, 'batch_size': 32, 'attn': 'prob', 'distil': True, 'mix': True}, lr=5.72304562512002e-05, optimizer=AdamW, lr_policy=clr
[2025-10-05 04:53:41,865][root][INFO] - GPU Memory: 1.06 GB used out of 8.00 GB
[2025-10-05 04:53:41,865][root][INFO] - Performed memory cleaning.
[2025-10-05 04:53:41,866][root][INFO] - GPU Memory: 1.06 GB used out of 8.00 GB
[2025-10-05 04:53:41,870][root][INFO] - Trial 16: X_raw shape=(1917, 1671), Y_raw shape=(1917, 424), NaNs X=0, Y=0
[2025-10-05 04:53:41,870][root][INFO] - Trial 17: X_raw shape=(1917, 1671), Y_raw shape=(1917, 424), NaNs X=0, Y=0
[2025-10-05 04:53:41,870][root][INFO] - Building Informer datasets with input_len=96, output_len=5, label_len=64, use_time_feats=True
[2025-10-05 04:53:41,870][root][INFO] - Building Informer datasets with input_len=96, output_len=5, label_len=64, use_time_feats=True
[2025-10-05 04:53:42,485][root][INFO] - WindowDataset created: Xw shape=(1817, 96, 1671), Yw shape=(1817, 5, 424), invalid_windows: 0
[2025-10-05 04:53:42,500][root][INFO] - WindowDataset created: Xw shape=(1817, 96, 1671), Yw shape=(1817, 5, 424), invalid_windows: 0
[2025-10-05 04:53:48,898][root][INFO] - InformerWindowDataset created: Xw shape=(1817, 96, 1671), x_dec shape=(1817, 69, 424), Yw shape=(1817, 5, 424), mark_enc shape=(1817, 96, 384), invalid_windows=0
[2025-10-05 04:53:48,898][root][INFO] - Informer dataset created: X_enc shape=(1817, 96, 1671), x_dec shape=(1817, 69, 424), Y shape=(1817, 5, 424), mark_enc shape=(1817, 96, 384)
[2025-10-05 04:53:48,907][root][INFO] - InformerWindowDataset created: Xw shape=(1817, 96, 1671), x_dec shape=(1817, 69, 424), Yw shape=(1817, 5, 424), mark_enc shape=(1817, 96, 384), invalid_windows=0
[2025-10-05 04:53:48,917][root][INFO] - Informer dataset created: X_enc shape=(1817, 96, 1671), x_dec shape=(1817, 69, 424), Y shape=(1817, 5, 424), mark_enc shape=(1817, 96, 384)
[2025-10-05 04:53:49,628][root][INFO] - Informer DataLoaders created: train=1364, val=272, test=181
[2025-10-05 04:53:49,629][root][INFO] - Trial 16: Creating InTrainer with device=cuda
[2025-10-05 04:53:49,629][root][INFO] - InTrainer initialized with model_kwargs: {'input_size': 1671, 'enc_in': 1671, 'dec_in': 424, 'c_out': 424, 'seq_len': 96, 'label_len': 64, 'out_len': 5, 'd_model': 384, 'n_heads': 4, 'e_layers': 3, 'd_layers': 1, 'd_ff': 512, 'dropout': 0.15000000000000002, 'attn': 'prob', 'embed': 'fixed', 'freq': 'd', 'activation': 'gelu', 'distil': True, 'mix': True, 'device': 'cuda'}
[2025-10-05 04:53:49,652][root][INFO] - Informer DataLoaders created: train=1364, val=272, test=181
[2025-10-05 04:53:49,652][root][INFO] - Trial 17: Creating InTrainer with device=cuda
[2025-10-05 04:53:49,652][root][INFO] - InTrainer initialized with model_kwargs: {'input_size': 1671, 'enc_in': 1671, 'dec_in': 424, 'c_out': 424, 'seq_len': 96, 'label_len': 64, 'out_len': 5, 'd_model': 384, 'n_heads': 4, 'e_layers': 3, 'd_layers': 1, 'd_ff': 512, 'dropout': 0.15000000000000002, 'attn': 'prob', 'embed': 'fixed', 'freq': 'd', 'activation': 'gelu', 'distil': True, 'mix': True, 'device': 'cuda'}
[2025-10-05 04:53:49,837][root][INFO] - Trainer initialized with device=cpu
[2025-10-05 04:53:49,848][root][INFO] - Trainer initialized with device=cpu
[2025-10-05 04:53:50,297][root][ERROR] - Trial 16 failed: einsum(): subscript h has size 26 for operand 1 which does not broadcast with previously seen size 69
Traceback (most recent call last):
  File "/root/vscode/portfolio/mitsui_2025/src/study.py", line 202, in objective_informer
    val_loss = trainer.train(train_loader=train_loader, val_loader=val_loader)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/trainer.py", line 673, in train
    pred = self.model(x_enc, x_mark_enc=x_mark_enc, x_dec=x_dec, x_mark_dec=x_mark_dec)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/models.py", line 1611, in forward
    dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/models.py", line 1471, in forward
    x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/models.py", line 1457, in forward
    x = x + self.dropout(self.cross_attention(x, cross, cross, attn_mask=cross_mask)[0])
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/models.py", line 1386, in forward
    out, attn = self.attention(Q, K, V, attn_mask=attn_mask)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/models.py", line 1514, in forward
    scores = torch.einsum("blhe,bshe->blhs", queries, keys)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/functional.py", line 402, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: einsum(): subscript h has size 26 for operand 1 which does not broadcast with previously seen size 69

[2025-10-05 04:53:50,297][root][ERROR] - Trial 17 failed: einsum(): subscript h has size 26 for operand 1 which does not broadcast with previously seen size 69
Traceback (most recent call last):
  File "/root/vscode/portfolio/mitsui_2025/src/study.py", line 202, in objective_informer
    val_loss = trainer.train(train_loader=train_loader, val_loader=val_loader)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/trainer.py", line 673, in train
    pred = self.model(x_enc, x_mark_enc=x_mark_enc, x_dec=x_dec, x_mark_dec=x_mark_dec)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/models.py", line 1611, in forward
    dec_out = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/models.py", line 1471, in forward
    x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/models.py", line 1457, in forward
    x = x + self.dropout(self.cross_attention(x, cross, cross, attn_mask=cross_mask)[0])
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/models.py", line 1386, in forward
    out, attn = self.attention(Q, K, V, attn_mask=attn_mask)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/vscode/portfolio/mitsui_2025/src/models.py", line 1514, in forward
    scores = torch.einsum("blhe,bshe->blhs", queries, keys)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/anaconda3/envs/mitsui/lib/python3.11/site-packages/torch/functional.py", line 402, in einsum
    return _VF.einsum(equation, operands)  # type: ignore[attr-defined]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: einsum(): subscript h has size 26 for operand 1 which does not broadcast with previously seen size 69

[2025-10-05 04:53:50,863][root][INFO] - GPU Memory: 1.06 GB used out of 8.00 GB
[2025-10-05 04:53:50,863][root][INFO] - GPU Memory: 1.06 GB used out of 8.00 GB
[2025-10-05 04:53:51,142][root][INFO] - Performed memory cleaning.
[2025-10-05 04:53:51,417][root][INFO] - Performed memory cleaning.
[2025-10-05 04:53:51,453][root][WARNING] - No completed trials for INF. All trials were pruned or failed. Trials: 18
[2025-10-05 04:53:51,456][root][INFO] - Trial states: [3, 3, 3, 3, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
