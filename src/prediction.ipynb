{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95b6f28f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'grpc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle_evaluation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mitsui_inference_server\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Import custom modules (adjust paths if needed)\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_ckp, FEDForecaster, load_timesfm_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode/portfolio/mitsui_2025/src/kaggle_evaluation/mitsui_inference_server.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle_evaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtemplates\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmitsui_gateway\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mMitsuiInferenceServer\u001b[39;00m(kaggle_evaluation.core.templates.InferenceServer):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode/portfolio/mitsui_2025/src/kaggle_evaluation/core/templates.py:13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpl\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle_evaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase_gateway\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle_evaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrelay\u001b[39;00m\n\u001b[32m     17\u001b[39m _initial_import_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode/portfolio/mitsui_2025/src/kaggle_evaluation/core/base_gateway.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msocket\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gaierror\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, final, List, Optional, Tuple, Union\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgrpc\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'grpc'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Logging Setup\n",
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import numpy as np\n",
    "import logging\n",
    "import json\n",
    "import joblib\n",
    "from kaggle_evaluation import mitsui_inference_server\n",
    "\n",
    "# Import custom modules (adjust paths if needed)\n",
    "from models import load_ckp, FEDForecaster, load_timesfm_model\n",
    "from adapters import FEDAdapter, TimesFMAdapter\n",
    "from utils import weighted_ensemble\n",
    "from dataprep import impute_features_with_staleness  # Added for preprocessing\n",
    "\n",
    "# Constants\n",
    "NUM_TARGET_COLUMNS = 424\n",
    "CHECKPOINT_DIR = \"../checkpoints/\"\n",
    "\n",
    "# Global state for persistence\n",
    "loaded = False\n",
    "adapters = []  # List of (adapter, model_type, checkpoint_path) tuples\n",
    "history = pl.DataFrame()  # Persistent price/feature history\n",
    "feature_cols_list = []  # Per-model feature columns\n",
    "target_cols = None  # Shared target columns\n",
    "input_len = 512  # Default; updated per model\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('inference_log.log'), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "logging.info(\"Environment setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fbc562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    test: pl.DataFrame,\n",
    "    label_lags_1_batch: pl.DataFrame,\n",
    "    label_lags_2_batch: pl.DataFrame,\n",
    "    label_lags_3_batch: pl.DataFrame,\n",
    "    label_lags_4_batch: pl.DataFrame,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensemble predictions from FED and TimesFM models with preprocessing.\n",
    "    \"\"\"\n",
    "    global loaded, adapters, history, feature_cols_list, target_cols, input_len\n",
    "\n",
    "    if not loaded:\n",
    "        try:\n",
    "            checkpoints = [\n",
    "                (\"FED\", \"FED_trial7_20251004_185519.pt\", FEDForecaster, FEDAdapter),\n",
    "                (\"TimesFM\", \"timesfm/finetuned_target_0/finetuned.pt\", None, TimesFMAdapter)\n",
    "            ]\n",
    "\n",
    "            for model_type, ckpt_file, model_class, adapter_class in checkpoints:\n",
    "                ckpt_path = os.path.join(CHECKPOINT_DIR, ckpt_file)\n",
    "                if not os.path.exists(ckpt_path):\n",
    "                    logging.error(f\"Checkpoint {ckpt_path} not found\")\n",
    "                    continue\n",
    "\n",
    "                if model_type == \"FED\":\n",
    "                    model, scaler, feat_cols, tgt_cols, model_params = load_ckp(\n",
    "                        dir=os.path.dirname(ckpt_path),\n",
    "                        model_class=model_class,\n",
    "                        model_kwargs=model_params if os.path.exists(os.path.join(os.path.dirname(ckpt_path), \"model_params.json\")) else {},\n",
    "                        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "                    )\n",
    "                    adapter = adapter_class(model, scaler, tgt_cols)\n",
    "                    current_input_len = model_params.get('input_len', input_len)\n",
    "                else:  # TimesFM\n",
    "                    model = load_timesfm_model(\n",
    "                        model_name=\"google/timesfm-1.0-200m-pytorch\",\n",
    "                        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                        horizon_len=1,\n",
    "                        context_len=512,\n",
    "                        batch_size=32\n",
    "                    )\n",
    "                    state_dict = torch.load(ckpt_path, map_location=model.device)\n",
    "                    model.load_state_dict(state_dict)\n",
    "                    model.eval()\n",
    "                    tgt_cols_path = os.path.join(os.path.dirname(ckpt_path), \"target_cols.json\")\n",
    "                    tgt_cols = json.load(open(tgt_cols_path)) if os.path.exists(tgt_cols_path) else [f\"target_{i}\" for i in range(NUM_TARGET_COLUMNS)]\n",
    "                    feat_cols = tgt_cols\n",
    "                    scaler = joblib.load(os.path.join(os.path.dirname(ckpt_path), \"scaler.pkl\")) if os.path.exists(os.path.join(os.path.dirname(ckpt_path), \"scaler.pkl\")) else None\n",
    "                    adapter = adapter_class(model, scaler, tgt_cols, context_length=512, horizon_length=1, freq_type=0)\n",
    "                    current_input_len = 512\n",
    "\n",
    "                if target_cols is None:\n",
    "                    target_cols = tgt_cols\n",
    "                elif target_cols != tgt_cols:\n",
    "                    logging.warning(f\"Target cols mismatch in {ckpt_file}; expected {target_cols}, got {tgt_cols}\")\n",
    "                    continue\n",
    "\n",
    "                adapters.append((adapter, model_type, ckpt_path))\n",
    "                feature_cols_list.append(feat_cols)\n",
    "                input_len = max(input_len, current_input_len)\n",
    "                logging.info(f\"Loaded {model_type} from {ckpt_path}, features: {len(feat_cols)}, targets: {len(tgt_cols)}\")\n",
    "\n",
    "            if not adapters:\n",
    "                raise ValueError(\"No valid models loaded for ensembling\")\n",
    "            loaded = True\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Model loading failed: {str(e)}\")\n",
    "            return pl.DataFrame({f'target_{i}': [0.0] for i in range(NUM_TARGET_COLUMNS)})\n",
    "\n",
    "    # Preprocess test batch\n",
    "    test_pd = test.to_pandas()\n",
    "    X_imputed, _, _ = impute_features_with_staleness(test_pd, date_col=\"date_id\", cap_days=None)\n",
    "    test_processed = pl.from_pandas(X_imputed)\n",
    "\n",
    "    # Append to history\n",
    "    history = pl.concat([history, test_processed])\n",
    "\n",
    "    # Ensure sufficient history\n",
    "    if len(history) < input_len:\n",
    "        pad_rows = input_len - len(history)\n",
    "        pad_dict = {col: [0.0] * pad_rows for col in history.columns if col != 'date_id'}\n",
    "        if 'date_id' in history.columns:\n",
    "            last_date = history['date_id'][-1] if len(history) > 0 else 0\n",
    "            pad_dict['date_id'] = list(range(last_date - pad_rows, last_date))\n",
    "        pad_df = pl.DataFrame(pad_dict)\n",
    "        history = pl.concat([pad_df, history])\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = []\n",
    "    date_id = history['date_id'][-1] if 'date_id' in history.columns else None\n",
    "    for (adapter, model_type, ckpt_path), feat_cols in zip(adapters, feature_cols_list):\n",
    "        try:\n",
    "            recent_history = history.tail(input_len)\n",
    "            X_hist_pd = recent_history.select(feat_cols + ['date_id']).to_pandas()\n",
    "            pred, metadata = adapter.predict_next(X_hist_pd, Y_hist=None, input_len=input_len, lag=0)\n",
    "            if not np.isfinite(pred).all():\n",
    "                logging.warning(f\"Non-finite predictions from {model_type}, replacing with zeros\")\n",
    "                pred = np.zeros(NUM_TARGET_COLUMNS, dtype=float)\n",
    "            predictions.append(pred)\n",
    "            logging.info(f\"{model_type} predicted for date_id {date_id}: {pred[:5]}...\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"{model_type} prediction failed: {str(e)}\")\n",
    "            predictions.append(np.zeros(NUM_TARGET_COLUMNS, dtype=float))\n",
    "\n",
    "    # Ensemble predictions\n",
    "    weights = np.ones(len(predictions)) / len(predictions)\n",
    "    final_pred = weighted_ensemble(*predictions, w=weights)\n",
    "\n",
    "    # Convert to pl.DataFrame\n",
    "    pred_dict = {f'target_{i}': [float(final_pred[i])] for i in range(NUM_TARGET_COLUMNS)}\n",
    "    return pl.DataFrame(pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503506f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'kaggle_evaluation' has no attribute 'mitsui_inference_server'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Inference server\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m inference_server = \u001b[43mkaggle_evaluation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmitsui_inference_server\u001b[49m.MitsuiInferenceServer(predict)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.getenv(\u001b[33m'\u001b[39m\u001b[33mKAGGLE_IS_COMPETITION_RERUN\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      5\u001b[39m     inference_server.serve()\n",
      "\u001b[31mAttributeError\u001b[39m: module 'kaggle_evaluation' has no attribute 'mitsui_inference_server'"
     ]
    }
   ],
   "source": [
    "# Inference server\n",
    "inference_server = mitsui_inference_server.MitsuiInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/mitsui-commodity-prediction-challenge/',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6a8c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85b34e24",
   "metadata": {},
   "source": [
    "----\n",
    "All-in-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ae3a19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kaggle_evaluation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkaggle_evaluation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmitsui_inference_server\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Import your custom modules (adjust paths as needed; assuming they are in /root/src or current dir)\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_ckp, LSTMForecaster  \u001b[38;5;66;03m# Replace LSTMForecaster with your model class if different\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'kaggle_evaluation'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "import joblib\n",
    "import kaggle_evaluation.mitsui_inference_server\n",
    "\n",
    "# Import your custom modules (adjust paths as needed; assuming they are in /root/src or current dir)\n",
    "from models import load_ckp, LSTMForecaster  # Replace LSTMForecaster with your model class if different\n",
    "from adapters import LSTMAdapter  # Replace with your specific adapter (e.g., FEDAdapter, TCNAdapter)\n",
    "from configs import TrainConfig  # If needed for defaults\n",
    "\n",
    "NUM_TARGET_COLUMNS = 424\n",
    "\n",
    "# Global state for persistence across calls (loaded once in first predict call)\n",
    "loaded = False\n",
    "adapter = None\n",
    "history = pl.DataFrame()  # Persistent price/feature history (appended per batch)\n",
    "feature_cols = None  # Will be loaded from checkpoint\n",
    "target_cols = None  # Will be loaded from checkpoint\n",
    "input_len = 64  # Default; will be updated from model_params if available\n",
    "\n",
    "# Set up basic logging (optional, for debugging; logs to file/stderr)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.FileHandler('inference_log.log'), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def predict(\n",
    "    test: pl.DataFrame,\n",
    "    label_lags_1_batch: pl.DataFrame,\n",
    "    label_lags_2_batch: pl.DataFrame,\n",
    "    label_lags_3_batch: pl.DataFrame,\n",
    "    label_lags_4_batch: pl.DataFrame,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Inference function: Predicts all 424 targets for the current date_id batch.\n",
    "    - Loads model/adapter once (in first call, no time limit).\n",
    "    - Appends new test features (prices) to persistent history.\n",
    "    - Prepares X_hist from history (last input_len rows).\n",
    "    - Uses adapter to predict next targets (forecast for current date_id).\n",
    "    - Ignores lagged labels (as current code doesn't use past targets as features).\n",
    "    - Returns pl.DataFrame with one row: target_0 to target_423.\n",
    "    \"\"\"\n",
    "    global loaded, adapter, history, feature_cols, target_cols, input_len\n",
    "\n",
    "    if not loaded:\n",
    "        try:\n",
    "            # Load checkpoint (adjust path; e.g., select best model file)\n",
    "            ckpt_dir = \"/root/checkpoints/\"  # Or os.environ.get('CHECKPOINT_DIR', './checkpoints/')\n",
    "            model_class = LSTMForecaster  # Replace with your model (e.g., FEDForecaster)\n",
    "            model_kwargs = {}  # Base kwargs; load_ckp will use model_params.json if present\n",
    "\n",
    "            model, scaler, feature_cols, target_cols, model_params = load_ckp(\n",
    "                dir=ckpt_dir,\n",
    "                model_class=model_class,\n",
    "                model_kwargs=model_kwargs,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            )\n",
    "\n",
    "            # Update input_len from model_params (if saved)\n",
    "            input_len = model_params.get('input_len', input_len)\n",
    "\n",
    "            # Create adapter (replace LSTMAdapter with your model's adapter)\n",
    "            adapter = LSTMAdapter(model, scaler, target_cols)\n",
    "\n",
    "            loaded = True\n",
    "            logging.info(f\"Model loaded from {ckpt_dir}. Feature cols: {len(feature_cols)}, Target cols: {len(target_cols)}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Model loading failed: {str(e)}\")\n",
    "            # Fallback to dummy predictions on error\n",
    "            return pl.DataFrame({f'target_{i}': [0.0] for i in range(NUM_TARGET_COLUMNS)})\n",
    "\n",
    "    # Append current test batch to history (assumes test has date_id and feature columns)\n",
    "    history = pl.concat([history, test])\n",
    "\n",
    "    # Ensure history has enough rows; pad with zeros if too short (rare, but defensive)\n",
    "    if len(history) < input_len:\n",
    "        pad_rows = input_len - len(history)\n",
    "        pad_dict = {col: [0.0] * pad_rows for col in history.columns if col != 'date_id'}\n",
    "        if 'date_id' in history.columns:\n",
    "            last_date = history['date_id'][-1] if len(history) > 0 else 0\n",
    "            pad_dict['date_id'] = list(range(last_date - pad_rows, last_date))\n",
    "        pad_df = pl.DataFrame(pad_dict)\n",
    "        history = pl.concat([pad_df, history])\n",
    "\n",
    "    # Prepare X_hist as pd.DataFrame for adapter (select last input_len rows, feature_cols + date_id)\n",
    "    if 'date_id' not in history.columns:\n",
    "        history = history.with_columns(pl.Series(name='date_id', values=range(len(history))).alias('date_id'))\n",
    "    recent_history = history.tail(input_len)\n",
    "    X_hist_pd = recent_history.select(feature_cols + ['date_id']).to_pandas()\n",
    "\n",
    "    # Predict (lag=0 since predicting for current; adjust if your setup uses lag for horizon)\n",
    "    try:\n",
    "        pred, metadata = adapter.predict_next(X_hist_pd, Y_hist=None, input_len=input_len, lag=0)\n",
    "        logging.info(f\"Predicted for date_id {metadata.get('date_id')}: {pred[:5]}...\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Prediction failed: {str(e)}\")\n",
    "        pred = np.zeros(NUM_TARGET_COLUMNS, dtype=float)\n",
    "\n",
    "    # Convert predictions to pl.DataFrame (one row)\n",
    "    pred_dict = {f'target_{i}': [pred[i]] for i in range(NUM_TARGET_COLUMNS)}\n",
    "    return pl.DataFrame(pred_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae6d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the inference server\n",
    "inference_server = kaggle_evaluation.mitsui_inference_server.MitsuiInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/mitsui-commodity-prediction-challenge/',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52295435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd62358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16518a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mitsui",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
